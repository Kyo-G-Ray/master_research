\section{物体認識実験}
本章では，
物体認識の現状, 手法及びオープンソースのコンピュータビジョンライブラリである Open Source Computer Vision Library 通称 OpenCV を用いた物体認識実験の結果について述べる.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{物体認識}
ニューラルネットワークとは，
人間の脳の仕組みから着想を得たものであり，
神経回路網をコンピュータ上で表現しようと作られた数理的モデルである．
人間の脳はニューロンと呼ばれる神経細胞の結びつきで情報伝達や記憶の定着が行われており，
その人間の脳神経系の持つ強力な学習能力を数学的に応用することにより，
画像認識や音声認識などへの利用が期待され研究が行われている．\\
　通常ニューラルネットワークは，
入力層，
出力層，
隠れ層から構成されており，
これらの層と層の間にニューロン同士のつながりを示す $W$ があり，
これを重みと呼ぶ．
ニューラルネットワークの隠れ層は多層にすることができ，
隠れ層が多数存在する多層構造のニューラルネットワークのことをディープラーニングと呼ぶ．
そのディープラーニングの主力となっているものの一つに畳み込みニューラルネットワークが有り，
畳み込み層とプーリング層を 1 つのペアーとして，
それらが複数回重ね合わせて構成される順方向性ニューラルネットワークである．
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure*}[!b]
%  \begin{center}
%    \includegraphics[scale=0.5]{./chapter2_001.eps}
%    \caption{畳み込みニューラルネットワークの基本構造}
%  \end{center}
%\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{畳み込み層}
畳み込みニューラルネットワークに入力としての画像が与えられたとき，
その画像の特徴量を抽出する操作のことを畳み込みと呼ぶ．\\
　畳み込み層は単純型細胞をモデルに考えられたものであり，
元の画像にフィルタをかけて元の画像よりも少し小さい特徴マップを出力する，
またこのフィルタは画像全体をスライドするので特徴がどこにあっても抽出することができ，
その際にフィルタの数だけ特徴マップが出力される．\\
ここで例として，
対象とする画像を$X × Y$ Pixels の RGB の階調値とし，
k 番目の階調の素子（ i ，j ）の画素値を$I^{k}_{ij}$ とする．
ただし，
$k = 1$ が R，$k = 2$ が G，$k = 3$ が B とする．
最初の層の畳み込み層の $a$ 番目のフィルターの$(i,\ j)$ 番目の素子の内部状態
$y^{(1)(a)}_{ij}$は，
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  y^{(1) (a)}_{ij} = \sum_{k=1}^3
  \left(\sum_{x \in W} \sum_{y \in W}
         w^{(1)(a)(k)}_{ij} I^{(k)}_{i + x,\ j + y}  + b^{(1)(a)(k)}_{ij}
         \right)
  %%%%%
  \label{eq:convolution1st}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
その出力
$\tilde{y}_{ij}^{(1)(a)}$は，
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  \tilde{y}_{ij}^{(1)(a)} = \max \left(y^{(1) (a)}_{ij},\ 0 \right)
  %%%%%
  \label{eq:OutputConvolution}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
$\ell$ 番目の層の畳み込み層の内部状態
$y^{(\ell)(a)}_{ij}$ は，
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  y^{(\ell) (a)}_{ij} = \sum_{\alpha=1}^{N(\ell - 1)} \sum_{x \in W} \sum_{y \in W}
     w^{(\ell)(a,\ \alpha)}_{ij} z^{(\ell - 1)(\alpha)}_{i + x,\ j + y}
         + b^{(\ell)(a)}_{ij}
  %%%%%
  \label{eq:convolutionLth}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
で表される．
ここで，
$w^{(1)(a)(k)}_{ij}$ は入力層と畳み込み層間のシナプス結合加重，
$W$ は各素子が入力を受ける範囲を与える配置集合（受容野），
$b^{(1)(a)(k)}_{ij}$ は閾値である．
最終層（$L$）の内部状態 $y^{(L)}_{k}$ は，
前層のプーリング層の出力 $z^{(L - 1)(a)}_{ij}$ との全結合として，
以下のように与えられる．
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  y^{(L)}_{k} = \sum_{\alpha=1}^{N(L - 1)} \sum_{i} \sum_{j}
     w^{(L)(\alpha)}_{kij} z^{(L - 1)(\alpha)}_{ij}
         + b^{(L)_k}
   %%%%%
  \label{eq:OutputInternal}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
これらの内部状態は活性化関数と呼ばれる非線形関数を経て，
畳み込み層の出力としてプーリング層へ伝播する．
活性化関数の例として，
ステップ関数．
シグモイド関数，
ソフトマックス関数,
恒等関数,
 ReLU などが挙げられる．
ステップ関数は，
階段関数とも呼ばれる閾値を境にして出力が切り替わるようにする関数である．
シグモイド関数は，
式で表すと以下のように表される．
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  h(x) = \frac{1}{（1 + e^{-x}）}
  %%%%%%%%%%%%
  \label{eq:sigmoid}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
入力した値が大きいほど 1 に近づき，
入力した値が小さいほど 0 に近づく関数である．
ソフトマックス関数は，
式で表すと以下のように表される．
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  y_i = \frac{e^{x_i}}{\sum_{i=1}^{n}e^{x_i}}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
恒等関数は,受け取った値をそのまま出力したい場合に用いる関数である.
ReLU は 0 以下の値は,
0 にして 0 以上の値の場合にそのままの値を出力するという関数である.
ReLU を式で表すと以下のようになる.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
　f = max（ 0 , x ）
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
入力層が 6×6,畳み込みフィルタがフィルタが 3×3 のときの畳み込み操作の例を以下に示す.ただし入力を x,出力を y とし,畳み込みフィルタの重みを $w$ としたときの畳み込み層での点（a,b）についての情報量を求めたい場合を図に示す.その後に左上から右下への畳込みを考えストライドを 1 としたときの（a+1,b）での畳込みを図に示す.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!tp]
  \begin{center}
    \includegraphics[width=15cm,height=9cm]{chapter2_ab.eps}
    \caption{点（a,b）での畳み込み処理}
  \end{center}
  \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!bp]
  \begin{center}
    \includegraphics[width=15cm,height=9cm]{chapter2_a1b.eps}
    \caption{点（a+1,b）での畳み込み処理}
  \end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{プーリング層}
畳み込み層は単純型細胞をモデルに考えられたものであるが，
プーリング層は複雑型細胞をモデルにして作られたモデルである．\\
　画像から重要な情報を残して縮小する方法であり，
特徴マップからとある一部分の領域を取り出し，
そのうちの最大値を取り出す最大プーリング（ max pooling ）や，
特徴マップからとある一部分の領域を取り出し，
その領域の平均値を取る平均プーリング（ average pooling ）などがあるが，
画像認識の分野では最大プーリングが使われていることが多い．
また、プ−リング層の出力を$z^{(1)(a)}_{ij}$とすると以下のように表される．
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  z_{ij}^{(1)(a)} = \max_{x \in W,\ y\in W} \tilde{y}_{i + x,\ j + y}^{(1)(a)}
  %%%%%
  \label{eq:Pooling}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
最大プーリングと平均プーリングの例をそれぞれ図 2 と図 3 に示す．
畳み込み層やプーリング層で行われる手法にパディングというものがある．
パディングとは入力として与えられた画像のピクセルの周りにピクセルを配置し囲むという方法である．
パディングを行うことによるメリットは，
畳み込み層やプーリング層を経ることによって入力画像のサイズが小さくなっていくので，
パディングを行うことにより画像サイズを小さくしなくて済むという点と，
畳み込み層やプーリング層を経ることにより外側から縮小されていく画像にパディングを行うことで，
画像の端のピクセルも畳み込みの回数を増やすことができるので精度を高めることができるという点がある．
パディングの手法の一つにゼロパディングと呼ばれるピクセルの周りをゼロのピクセルで囲うというものがある,このゼロパディングの例を図 4 に示す.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!bp]
  \begin{center}
    \includegraphics[scale=0.5]{chapter2_003.eps}
  \end{center}
\caption{最大プーリングの例}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!hb]
  \begin{center}
    \includegraphics[scale=0.5]{chapter2_004.eps}
  \end{center}
\caption{平均プーリングの例}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!b]
  \begin{center}
    \includegraphics[scale=0.45]{chapter2_005.eps}
  \end{center}
\caption{ゼロパディングの例}
\end{figure}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{全結合層}
畳み込み層,プーリング層を経ても入力として与えられた画像の配列のままであるが,
これらの層を経て特徴データが取り出された入力として与えられた画像データを一つのノードに結合し,
活性化関数によって変換された値を出力する層である.\\
\subsubsection{CNN のモデル}
CNN の最初のモデルは LeCun たちにより提案された LeNet と呼ばれるものであり,その構造は畳み込み層とプーリング層が交互に並んでいる構造である.第一層の畳み込み層で 32×32 の入力データを 5×5  のフィルタで畳み込みを行い 28×28 の特徴マップを出力する.この出力を 2×2 のフィルタでプーリングを行い,次に 14×14 の出力にする.この特徴マップを 5×5 のフィルタで畳み込みを行い, 10×10 の特徴マップとしてプーリング層に出力する. 2×2 フィルタを持つプーリング層は 5×5 の特徴マップを出力して全結合層に入力信号として渡され,出力は活性化関数を通して 10 クラスの分類が行われる.\\
　14 年後の 2012 年にトロント大学の研究者である Alex Krizhevsky たちが提案したモデルが AlexNet と呼ばれる CNN で畳み込み層とプーリング層が交互に並ぶ構造である.ネットワーク構造を図に示す.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!t]
  \begin{center}
  \caption{AlexNet の層構造}
  \begin{tabular}{|c|c|c|} \hline
    順番 & 層 & カーネルサイズ \\ \hline
    1 & conv/4 & 11×11×3×96 \\ \hline
    2 & max-pooling & - \\ \hline
    3 & conv & 5×5×4×256 \\ \hline
    4 & max-pooling & - \\ \hline
    5 & conv & 3×3×256×384 \\ \hline
    6 & conv & 3×3×192×384 \\ \hline
    7 & conv & 3×3×192×256 \\ \hline
    8 & max-pooling & - \\ \hline
    9 & FC-4096 & 1×1×4096 \\ \hline
    10 & FC-4096 & 1×1×4096 \\ \hline
    11 & FC-1000 & 1×1×1000 \\ \hline
    - & softmax & - \\ \hline
  \end{tabular}
  \end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[!hb]
%  \begin{center}
%    \includegraphics[scale=0.7]{chapter2_Alex.eps}
%  \end{center}
%\caption{AlexNet の層構造}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
この AlexNet には五層の畳み込み層と三層のプーリング層が存在する.ネットワークの最終部分には分類識別のために三種類の全結合層（Fully Connected Layer : FC）が配置され,最後の層からは softmax 関数を通じて, 1000 クラスに対する確率分布が出力される.これは ImageNet と呼ばれる 1400 万枚以上の画像のデータセットに採用されている 1000 クラス分類に対応させるためである.また畳み込み層が二つに分割されている理由としては GPU の性能によるものである.\\
　オックスフォード大学の Visual Geometry Group という研究グループが開発したモデルに　VGG がある.畳み込み層とプーリング層から構成されており, AlexNet の進化系で層の深さを 16 層, 19 層にしたモデルである.非常に小さな 3×3 の畳み込みフィルタを用いて,ネットワークの深さを 16 層から 19 層に増加させることにより識別の精度を改善している.この小さなフィルタを持つ畳み込み層を 2 から 4 回連続して重ね,それをプーリング層でサイズを半分にすることを繰り返し行う構造が特徴である.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!t]
  \begin{center}
  \caption{VGG16 の層構造}
  \begin{tabular}{|c|c|c|} \hline
    順番 & 層 & カーネルサイズ \\ \hline
    1 & conv3-64 & 224×224×64 \\ \hline
    2 & conv3-64 & 224×224×64 \\ \hline
    - & max-pooling & 112×112×128 \\ \hline
    3 & conv3-128 & 112×124×128 \\ \hline
    4 & conv3-128 & 112×124×128 \\ \hline
    5 & conv3-128 & 112×124×128 \\ \hline
    - & max-pooling & 56×56×256 \\ \hline
    6 & conv3-256 & 56×156×256 \\ \hline
    7 & conv3-256 & 56×156×256 \\ \hline
    8 & conv3-256 & 56×156×256 \\ \hline
    - & max-pooling & 28×28×516 \\ \hline
    9 & conv3-512 & 28×28×516 \\ \hline
    10 & conv3-512 & 28×28×516 \\ \hline
    - & max-pooling & 28×28×516 \\ \hline
    11 & conv3-512 & 14×14×516 \\ \hline
    12 & conv3-512 & 14×14×516 \\ \hline
    13 & conv3-512 & 14×14×516 \\ \hline
    - & max-pooling & 7×7×516 \\ \hline
    14 & FC-4096 & 1×1×4096 \\ \hline
    15 & FC-4096 & 1×1×4096 \\ \hline
    16 & FC-1000 & 1×1×1000 \\ \hline
    - & softmax & 1×1×1000 \\ \hline
  \end{tabular}
  \end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[!hb]
%  \begin{center}
%   \includegraphics[scale=0.6]{chapter2_VGG16.eps}
%  \end{center}
%\caption{VGG16 の層構造}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 2014 年の ILSVRC（ImageNet Large Scale Visual Recognition Challenge）と呼ばれる 2010 年から始まった大規模画像認識の競技会で 2 位という成績を残したことでも有用性が実証され,更にシンプルなアーキテクチャであり実用に向いている.\\
　入力画像は 224×224 の RGB 画像であり,最初の 3×3 の畳み込み層でのフィルタのチャネル数は 64 で,マックスプーリングごとに 2 倍になり, 128, 256, 526と増加していき,出力は 1×1000 次元の配列である.また VGG16 のネットワークの構造を図に示す.また FC-4096 は並列次数 4096 の全結合層を表す.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[!t]
  \begin{center}
  \caption{GoogleNet の層構造}
  \begin{tabular}{|c|c|c|} \hline
    順番 & 層 & カーネルサイズ \\ \hline
    1 & conv7 & 112×112×64 \\ \hline
    2 & max-pooling3 & 56×56×64 \\ \hline
    3 & conv3 & 56×56×192 \\ \hline
    4 & max-pooling3 & 28×28×192 \\ \hline
    5 & inception & 28×28×256 \\ \hline
    6 & inception & 28×28×480 \\ \hline
    7 & max-pooling3 & 14×14×480 \\ \hline
    8 & inception & 14×14×512 \\ \hline
    9 & inception & 14×14×512 \\ \hline
    10 & inception & 14×14×512 \\ \hline
    11 & inception & 14×14×528 \\ \hline
    12 & inception & 14×14×832 \\ \hline
    13 & max-pooling3 & 7×7×832 \\ \hline
    14 & inception & 7×7×832 \\ \hline
    15 & inception & 7×7×1024 \\ \hline
    16 & avg-pooling7 & 1×1×1024 \\ \hline
    17 & dropout(40$％$) & 1×1×1024 \\ \hline
    18 & linear & 1×1×1000 \\ \hline
    19 & softmax & 1×1×1000 \\ \hline
  \end{tabular}
  \end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{figure}[!hb]
%  \begin{center}
%    \includegraphics[scale=0.6]{chapter2_GoogleNet.eps}
%  \end{center}
%\caption{GoogleNet の層構造}
%\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
次に先述した 2014 年の ILSVRC の優勝モデルである GoogleNet は基本的なネットワークの構造は CNN　と同様であるが,特徴としてネットワークの構造が縦方向だけでなく横方向にも深さを持つことが挙げられる.それは複数の畳み込み層やプーリング層から構成される Inception モジュールと呼ばれる小さなネットワークを作成し,これらを通常の畳み込み層のように重ねていき CNN を構成していると言える.Inception モジュールを組み込んだ GoogleNet のネットワークの構造を図に示す.\\
　ここで conv3d は 3×3 のフィルタを持つ畳み込み層であり, 112×112×64 は出力のサイズを意味している.また Inception モジュールで 5×5 の畳み込み層を 3×3 の畳み込み層を 2 つ重ねたものに変更し,改良を施したモデルのことを Inception-V2 と呼ぶ.
　さらに総構造を深くしたものの代表例として,ResNet と呼ばれる　Microsoft 社の Kaiming He の研究グループによって提案され, 2015 年の ILSVRC で優勝した CNN モデルが挙げられる.それまでのネットワーク構造ではあまり層を深くしすぎると性能が逆に落ちてしまうという問題があったがそれを残差学習構造と呼ばれるある層への入力をバイパスして層をまたぎ奥の層へと入力し,勾配の消失や発散を防ぎ超多層のネットワークの実現を可能にした.\\
　このように CNN の層の数を増やしよりディープな構造にしたものを DNN（Deep Neural Network）とも呼ぶ.
\subsubsection{CNN の学習方法}
機械学習には教師あり学習,
教師なし学習,
強化学習の三つの学習法がある.
教師あり学習とは,
学習データに正解ラベルをつけて学習する方法である.
分類や予測などは教師あり学習のクラスである.
それに対して教師なし学習は,
学習するデータに正解ラベルをつけずに学習する方法である.
こちらの教師なし学習は,
様々な入力を読み取っていくうちに自律的に対象を認識できるようになっていく学習方法であり,
教師あり学習に比べ実装難度が高い.
クラスタリングや次元削減などは教師なし学習のクラスである.
強化学習は価値を最大化するような行動を学習する方法である.
囲碁や将棋で目先の取れるコマがあったとしてもそれが敗北につながるような手だとしたら,
取らずに勝利に近づく手を打つことが強化学習の目的である.
強化学習のアルゴリズムには,
Q 学習,
Sarsa ,
モンテカルロ法などがある.\\
　Google DeepMind の開発した AlphaGo はコンピュータ囲碁プログラムであり,
2015 年に人間のプロ囲碁棋士をハンディキャップ無しで破った初のコンピュータ囲碁プログラムである.
膨大な数の対局データを教師あり学習として学習し,
さらに強化学習を行い勝つためのパターンを特徴量として学習させており,
その後コンピュータ同士を対局させ競わせていくことにより強力なコンピュータ囲碁プログラムが誕生した.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RNN（ Recurrent Neural Network ）の概要}
ニューラルネットワークの出力を別のニューラルネットワークの入力として利用するような再帰的構造を持ったニューラルネットワークのことを RNN（ Recurrent Neural Network ）と呼ぶ.\\
　時間を追って得られたデータを時系列データと呼び,
画像などのデータが一つのベクトル $x_n$ で表されるのに対し,
$(x(1),… x(t),… x(T))$ という T 個のデータが 1 つの入力データ群
このデータを用いて更に次に得られるであろうデータを予測する方法の一つである.
時系列データには,
気温の推移や降水量の変化などの気象データ,
株価や売上の推移などが挙げられ,
これらのデータは一般的なデータの集まりではなく,
データの並び自体に意味を持つ.\\
　時刻 $t$ での入力を $x(t)$ ,
入力層と隠れ層の間の重みを U ,
隠れ層と出力層の間の重みを W とすると隠れ層の値 $s(t)$ とネットワークの出力 $y(t)$ は,
以下のように表すことができる.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  s(t) = f(Ux(t) + Ws(t-1))
  %%%%%%%%%%%%
  \label{eq:RNN1}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  y(t) = g(Vh(t))
  %%%%%%%%%%%%
  \label{eq:RNN2}
\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
またRNNの基本構造の例を図 5 に示す.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[hb]
  \begin{center}
    \includegraphics[scale=0.4]{chapter2_006.eps}
    \caption{RNN の基本構造}
  \end{center}
  \end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LSTM（Long Short - Term Memory ）の概要}
　LSTM（ Long Short - Term Memory）は 1995 年に RNN の拡張として 1995 年に登場した時系列データを扱うモデルである.
Long Term Memory（長期記憶）と Short Term Memory（短期記憶）という神経科学における用語から取られており,
従来の RNN で学習できなかった長期依存を学習できるという特徴を持つ.
LSTM は RNN の中間層のユニットを LSTM block と呼ばれるメモリと 3 つのゲートを持つブロックに置き換えることで実現されている.
畳み込み層の内部状態を以下のように表す.
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{equation}
  \scriptsize
  y^{(\ell) (a)}_{ij}(t) = y^{(\ell) (a)}_{ij} +
  \sum_{\tau = 1}^T \sum_{\alpha=1}^{N(\ell)} \sum_{x \in W} \sum_{y \in W}
  v^{(\ell)(a,\ \alpha)}_{ij}
  y^{(\ell)(\alpha)}_{i + x,\ j + y}(t - \tau)
  %%
  \label{eq:LSTM}
\end{equation}\\
%%%%%%%%%%%%%%%%%
また,
LSTM の基本構造の例を図 6 に示す.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!hb]
  \begin{center}
    \includegraphics[scale=0.4]{chapter2_007.eps}
    \caption{LSTM の基本構造}
  \end{center}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
